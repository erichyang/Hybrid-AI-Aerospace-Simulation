# -*- coding: utf-8 -*-
"""Convergence.ipynb

Automatically generated by Colaboratory.

"""

from google.colab import drive
drive.mount('/content/mnt')

"""# Notebook Set Up

## Imports
"""

from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

from google.colab import drive

import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn import metrics
from keras import backend as K

import os
import datetime
import random

workingDirectory = 'mnt/My Drive/AI Project/CRM Convergence/'

def csv_to_np_array(file_path):
  return np.array(pd.read_csv(file_path))

def shuffle_split(a, seed):
  np.random.seed(seed)
  np.random.shuffle(a)
  temp = np.split(a, indices_or_sections=[0, int(0.2*len(a)+1)])
  return temp[1], temp[2]

"""## TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

"""# Regression

## Data Preprocessing
"""

x = csv_to_np_array(workingDirectory + 'parameters.csv')
y = csv_to_np_array(workingDirectory + 'solver_settings.csv')

y = y[:,3:7]

normx = preprocessing.MinMaxScaler()
normy = preprocessing.MinMaxScaler()
x = normx.fit_transform(x)
y = normy.fit_transform(y)

seed = random.randrange(0,99999)#3192861

xtest, xtrain = shuffle_split(x, seed)
ytest, ytrain = shuffle_split(y, seed)

#sd = [np.std(y[:,0]), np.std(y[:,1]), np.std(y[:,2]), np.std(y[:,3])]

"""## Model"""

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(25, activation = 'relu', input_shape = (2,), name='layer-0', kernel_regularizer=tf.keras.regularizers.l2(5)))
model.add(tf.keras.layers.Dense(50, activation = 'relu', name='layer-1', kernel_regularizer=tf.keras.regularizers.l2(5)))
model.add(tf.keras.layers.Dense(25, activation = 'relu', name='layer-2', kernel_regularizer=tf.keras.regularizers.l2(5)))
model.add(tf.keras.layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(4, activation = 'linear', name = 'output'))

#model.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
tb_cb = tf.keras.callbacks.TensorBoard(log_dir="logs/regression" + datetime.datetime.now().strftime("%H:%M"), histogram_freq=1)

"""## Training"""

#def step_decay(epoch):
#   initial_lrate = 0.1
#   drop = 0.5
#   epochs_drop = 1
#   lrate = initial_lrate * math.pow(drop,  
#           math.floor((1+epoch)/epochs_drop))
#   return lrate
#lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)
tf.keras.optimizers.Adam(learning_rate=0.1)

def avg_residual(arr):
  avg = tf.keras.backend.mean(arr)
  residuals = K.abs(avg-arr)
  return tf.keras.backend.mean(residuals)

def res(y, pred):
  return (tf.keras.losses.mean_absolute_error(y, pred))-(avg_residual(pred[:,0])-avg_residual(pred[:,1])-avg_residual(pred[:,2])-avg_residual(pred[:,3]))

#loss_fn = tf.keras.losses.mean_absolute_error

model.compile(optimizer = 'adam', loss = res, metrics = ['MAE'])

model.fit(x=xtrain,y=ytrain, epochs = 2000, verbose=2, callbacks=[tb_cb], initial_epoch= 0, validation_data=(xtest, ytest))

"""# Results"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/

model.save(workingDirectory+'model.h5')

model.predict(xtest)

ytest

#inversed
invtest = csv_to_np_array(workingDirectory + 'solver_settings.csv')[0:18,3:7]
invpred = normy.inverse_transform(model.predict(xtest))
invmse = np.array([metrics.mean_absolute_error(invtest[:,0], invpred[:,0]), metrics.mean_absolute_error(invtest[:,1], invpred[:,1]), metrics.mean_absolute_error(invtest[:,2], invpred[:,2]), metrics.mean_absolute_error(invtest[:,3], invpred[:,3])])
invmse

print(np.average(invmse))

print(invtest)
print(invpred)